{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 6\n",
    "from matplotlib import pyplot as plt\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загрузка файла\n",
    "z = zipfile.ZipFile(\"C:/Users/Оля/Documents/Универсл МАГА/HW Python/archive.zip\")\n",
    "df = pd.read_csv(z.open(\"IMDB Dataset.csv\"))\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обработка данных\n",
    "dct = {'positive':1, 'negative':0}\n",
    "df['sentiment_bin'] = df['sentiment'].map(dct)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обработка текста\n",
    "import nltk\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re \n",
    " def preproc(sentence):\n",
    "    sent_text = re.sub(r'[^\\w\\s]','', sentence)\n",
    "    words = sent_text.lower().split()\n",
    "    return(words)\n",
    " def senttxt(sent, tokenizer, remove_stopwords=False ):\n",
    "        raw_sentences = tokenizer.tokenize(oped.strip())\n",
    "        sentences = []\n",
    "        for raw_sentence in raw_sentences:\n",
    "            \n",
    "            sentences.append(preproc(raw_sentence))\n",
    "        \n",
    "        len(sentences)\n",
    "        return sentences\n",
    "txt_snt = df['review'].tolist()\n",
    "sentences = []\n",
    "for i in range(0,len(nyt_opeds)):\n",
    "    sent = txt_snt[i].replace(\"/.\", '')\n",
    "    sentences += senttxt(sent, tokenizer)\n",
    "    \n",
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(size=100, min_count=1)\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "\n",
    "class normword2vec():\n",
    "        \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        \n",
    "        X['review'] = X['review'].str.strip()\n",
    "        X['review'] = X['review'].str.lower()\n",
    "        X['review'] = X['review'].astype(str)\n",
    "        X['review'] = [re.sub(r'[^\\w\\s]', e) for e in X['review']]\n",
    "     \n",
    "        return X['review']\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "class MeanVect(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(word2vec.values())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(100)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(100)], axis=0)\n",
    "\n",
    "import sklearn\n",
    "import gensim.sklearn_api\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "          ('selectword2vec',  normword2vec()),\n",
    "      (\"word2vec\", MeanVect(w2v)),\n",
    "   \n",
    "     ('model_fitting',  xgb)]) \n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = df['y']\n",
    "X = df\n",
    "X_train,  X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.3,random_state = 2)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "pred = pipeline.predict(X_test)\n",
    "pd.crosstab(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Наивный Байес.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# для матрицы неточностей\n",
    "class_names = [\"positive\", \"negative\"]\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Матрица неточностей', cmap=plt.cm.Reds):    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float')/cm.sum(axis=0)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.min() + (cm.max() - cm.min()) * 2 / 3.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('Истина')\n",
    "    plt.xlabel('Предсказание')\n",
    "    plt.tight_layout()\n",
    "\n",
    "###\n",
    "train = df[[\"sentiment_bin\"]]\n",
    "target = df[\"review\"]\n",
    "\n",
    "gnb = GaussianNB()\n",
    "model = gnb.fit(train, target)\n",
    "gprob = gnb.predict_proba(train)\n",
    "gpred = gnb.predict(train)\n",
    "\n",
    "acc = accuracy_score(target, gpred)\n",
    "cnf = confusion_matrix(target, gpred)\n",
    "\n",
    "print(\"Точность = %f\" % acc)\n",
    "plot_confusion_matrix(cnf, class_names, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class CategorialNB\n",
    "\n",
    "def __init__(self, weight = None):\n",
    "        self.model_0 = [] \n",
    "        self.model_1 = []\n",
    "        \n",
    "        if weight is None:\n",
    "            self.weight = [0.5, 0.5] \n",
    "        else:\n",
    "            w = weight[0] + weight[1]\n",
    "            self.weight = [weight[0] / w, weight[1] / w]\n",
    "        \n",
    "    def fit(self, data, target):\n",
    "        data = np.array(data)\n",
    "        target = np.array(target)\n",
    "        \n",
    "        N = data.shape[0]\n",
    "        \n",
    "        feature_count = data.shape[1]\n",
    "\n",
    "        if data.shape[0] != target.shape[0]:\n",
    "            raise Exception(\"Invalid shapes of data vector and target vector\")\n",
    "            \n",
    "        if np.logical_not(np.isin(target, [0, 1])).any():\n",
    "            raise Exception(\"Invalid target vector\")\n",
    "\n",
    "        self.model_1 = [{} for _ in range(feature_count)]\n",
    "        self.model_0 = [{} for _ in range(feature_count)]\n",
    "            \n",
    "        mask_1 = target == 1\n",
    "        mask_0 = target == 0                    \n",
    "            \n",
    "        for i in range(feature_count):\n",
    "            cats, counts = np.unique(data[mask_1, i], return_counts=True)\n",
    "            probs = counts / N \n",
    "            for category, probability in zip(cats, probs):\n",
    "                self.model_1[i][category] = probability\n",
    "                \n",
    "            cats, counts = np.unique(data[mask_0, i], return_counts=True)\n",
    "            probs = counts / N             \n",
    "            for category, probability in zip(cats, probs):\n",
    "                self.model_0[i][category] = probability\n",
    "        #print(self.model)\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, data)\n",
    "        data = np.array(data)\n",
    "        result = np.zeros((data.shape[0], 2))\n",
    "        result[:, 0] = self.weight[0]\n",
    "        result[:, 1] = self.weight[1]\n",
    "        \n",
    "        for i, row in enumerate(data):\n",
    "            for j, feature in enumerate(row):\n",
    "                result[i, 0] = result[i, 0] * self.model_0[j][feature]\n",
    "                result[i, 1] = result[i, 1] * self.model_1[j][feature]        \n",
    "        return result / result.sum(axis=1)[:, None]\n",
    "    \n",
    "    def predict(self, data):\n",
    "        proba = self.predict_proba(data)                \n",
    "        mask = proba[:, 1] > proba[:, 0]\n",
    "        return mask.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Дерево решений.\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = df[[\"review\", \"sentiment\"]].values\n",
    "target = df['sentiment'].values\n",
    "train, test, target_train, target_test = train_test_split(   \n",
    "    data, target, \n",
    "    test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveTree:\n",
    "    def __init__(self, max_depth): \n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        self.p0 = None\n",
    "        self.p1 = None\n",
    "        self.size = None\n",
    "        self.target = None\n",
    "        self.entropy = None\n",
    "        \n",
    "        self.feature_num = None\n",
    "        self.feature_value = None\n",
    "        self.childs = None\n",
    "        \n",
    "    def set_depth(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "        if self.childs is not None:\n",
    "            self.childs[0].set_depth(max_depth - 1)\n",
    "            self.childs[1].set_depth(max_depth - 1)\n",
    "    \n",
    "    def _entropy(self, values):\n",
    "        p = values.sum() / values.shape[0]\n",
    "        q = 1.0 - p        \n",
    "        return - np.nan_to_num(p * np.log2(p)) - np.nan_to_num(q * np.log2(q))\n",
    "    \n",
    "    def print(self, names, tab=0):              \n",
    "        if self.childs is not None:\n",
    "            print(\"  \"*tab, \"[%4s == %2d][%6d]\" % (names[self.feature_num], self.feature_value, self.size), \n",
    "              \"%4.2f %4.2f %2d %7.5f\" %(self.p0, self.p1, self.target, self.entropy))\n",
    "            self.childs[0].print(names, tab+1)\n",
    "            self.childs[1].print(names, tab+1)\n",
    "        else:\n",
    "            print(\"  \"*tab, \"[____ == __][%6d]\" % self.size, \n",
    "              \"%4.2f %4.2f %2d %7.5f\" %(self.p0, self.p1, self.target, self.entropy))\n",
    "            \n",
    "    def _get_dot_code(self, names, name, parent=None):\n",
    "        content = \"\\n\"\n",
    "\n",
    "        if self.childs is not None:\n",
    "            content += '%s [label=\"%s == %s\\\\nS = %.3f\\\\nsamples = %d\\\\nprob = [%.2f, %.2f]\", fillcolor=\"#%X\"];\\n' % (\n",
    "                name, \n",
    "                names[self.feature_num], self.feature_value, \n",
    "                self.entropy, self.size, \n",
    "                self.p0, self.p1,\n",
    "                (0xe5813900 if self.p0 > self.p1 else 0x399de500) \n",
    "                + int(0xff * (self.p0 if self.p0 > self.p1 else self.p1))\n",
    "            )               \n",
    "        else:\n",
    "            content += '%s [label=\"S = %.3f\\\\nsamples = %d\\\\nprob = [%.2f, %.2f]\", fillcolor=\"#%X\"];\\n' % (\n",
    "                name, \n",
    "                self.entropy, self.size, \n",
    "                self.p0, self.p1,\n",
    "                (0xe5813900 if self.p0 > self.p1 else 0x399de500) \n",
    "                + int(0xff * (self.p0 if self.p0 > self.p1 else self.p1))\n",
    "            )               \n",
    "            \n",
    "        if parent is not None:\n",
    "            content += \"%s -> %s;\" % (parent, name)\n",
    "            \n",
    "        if self.childs is not None:\n",
    "            content += self.childs[0]._get_dot_code(names, name + \"f\", name)\n",
    "            content += self.childs[1]._get_dot_code(names, name + \"t\", name)\n",
    "            \n",
    "        return content\n",
    "            \n",
    "    def to_dot(self, filename, names):\n",
    "        f = open(filename, \"w\")\n",
    "        content = self._get_dot_code(names, \"root\", None)\n",
    "        f.write(\"digraph Tree {\\n\")\n",
    "        f.write('\\tnode [shape=box, style=\"filled\", color=\"black\"];\\n')\n",
    "        f.write(content)\n",
    "        f.write(\"}\")\n",
    "        f.close()        \n",
    "        \n",
    "    def _predict_proba(self, features):\n",
    "        if self.childs is None or self.max_depth <= 0:\n",
    "            return self.p0, self.p1\n",
    "        \n",
    "        if features[self.feature_num] == self.feature_value:\n",
    "            return self.childs[1]._predict_proba(features)\n",
    "        else:\n",
    "            return self.childs[0]._predict_proba(features)\n",
    "        \n",
    "    def _predict(self, features):\n",
    "        if self.childs is None or self.max_depth <= 0:\n",
    "            return self.target\n",
    "        \n",
    "        if features[self.feature_num] == self.feature_value:\n",
    "            return self.childs[1]._predict(features)\n",
    "        else:\n",
    "            return self.childs[0]._predict(features)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        data = np.array(data)\n",
    "        result = np.zeros(data.shape[0])\n",
    "        for i, features in enumerate(data):\n",
    "            result[i] = self._predict(features)\n",
    "        return result\n",
    "    \n",
    "    def predict_proba(self, data):\n",
    "        data = np.array(data)\n",
    "        result = np.zeros( (data.shape[0], 2) )\n",
    "        for i, features in enumerate(data):\n",
    "            result[i] = self._predict_proba(features)\n",
    "        return result\n",
    "    \n",
    "    def fit(self, data, target):\n",
    "        data = np.array(data)\n",
    "        target = np.array(target)\n",
    "    \n",
    "        mask = target == 0\n",
    "        self.size = target.shape[0]\n",
    "        self.p0 = mask.sum() / self.size\n",
    "        self.p1 = 1.0 - self.p0\n",
    "        self.target = 1 if self.p1 > self.p0 else 0\n",
    "        self.entropy = self._entropy(target)        \n",
    "        \n",
    "        if self.entropy == 0:\n",
    "            return\n",
    "                     \n",
    "        n_features = data.shape[1]        \n",
    "        features = [np.unique(data[:,i]) for i in range(n_features)]\n",
    "        \n",
    "        split_best = None\n",
    "        split_feature = None\n",
    "        split_feature_value = None\n",
    "        split_mask_true = None\n",
    "        split_mask_false = None\n",
    "        \n",
    "        for i, feature in enumerate(features):\n",
    "            if len(feature) < 2:\n",
    "                continue\n",
    "                \n",
    "            for fv in feature:\n",
    "                mask = data[:, i] == fv\n",
    "                not_mask = np.logical_not(mask)\n",
    "                \n",
    "                S_true = self._entropy(target[mask])\n",
    "                S_false = self._entropy(target[not_mask])\n",
    "                \n",
    "                p_true = mask.sum() / mask.shape[0]\n",
    "                p_false = 1.0 - p_true\n",
    "                \n",
    "                dS = p_true * S_true + p_false * S_false\n",
    "                \n",
    "                if split_best is None or split_best > dS:\n",
    "                    split_best = dS\n",
    "                    split_feature = i\n",
    "                    split_feature_value = fv\n",
    "                    split_mask_true = mask\n",
    "                    split_mask_false = not_mask\n",
    "\n",
    "        if split_best is None:\n",
    "            return\n",
    "                   \n",
    "        self.feature_num = split_feature\n",
    "        self.feature_value = split_feature_value\n",
    "\n",
    "        self.childs = [RecursiveTree(self.max_depth - 1), RecursiveTree(self.max_depth - 1)]\n",
    "        self.childs[0].fit(data[split_mask_false], target[split_mask_false])\n",
    "        self.childs[1].fit(data[split_mask_true], target[split_mask_true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree = RecursiveTree(3)\n",
    "tree.fit(train, target_train)\n",
    "pv = tree.predict_proba(test)\n",
    "yv = tree.predict(test)\n",
    "\n",
    "print(\"Accuracy = \", accuracy_score(target_test, yv))\n",
    "\n",
    "tree.to_dot(\"graph.dot\", names=[\"review\", \"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpng \"graph.dot\" -o \"graph.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Метод k - ближайших соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Линейные модели"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
